While quantum physics research was active in the 20th century, the primary concern was how to use quantum mechanics in computers. Moor's law states that the amount of computations that classical computers can perform doubles every 18 months as the size of transistors significantly shrinks; as a result, we can employ more transistors to have more operations per second. Although this law was established, scientists could only lower the size of transistors around 2005 because each transistor has a high temperature that harms the box that houses all of these transistors. Therefore, the quantum computer was the best solution to tackling this issue. Researchers found that by directly manipulating quantum systems, they might circumvent the challenges of simulating quantum events on ordinary computers. As a result, quantum computing was developed, which uses quantum systems to speed up calculations. In 1994, Peter Shor created the first effective quantum algorithm, which paved the way for an endless array of quantum algorithms to solve "oracle" issues. According to Knill, E. (2010), Shor's method may be used to crack cryptographic codes, which are often used for online transactions and communication. Thus, building quantum computers would be helpful to, required, and technically feasible in some respects. Quantum computing is the computational science that uses the principles of quantum mechanics to execute certain calculations. A quantum computer stores its data in units known as qubits which are the counterparts of a bit in classical computers.
